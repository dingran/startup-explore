{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "startup script successful\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "from __future__ import division\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas as pd\n",
    "import sys\n",
    "from IPython.core.display import display, HTML\n",
    "sns.set_context({'figure.figsize': [12, 7]})\n",
    "sns.set_context(rc={'lines.markeredgewidth': 0.1})\n",
    "sns.set_style('white')\n",
    "sns.set_color_codes()\n",
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12) \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import glob\n",
    "import os\n",
    "print 'startup script successful'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crunch_data=pd.read_csv('overall_company_list.csv', index_col=0)\n",
    "kickstart_data=pd.read_csv('proj_list_tech_top200pages_full.csv')\n",
    "angellist_data=pd.read_csv('results_so_far_2016101710pm.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acq_file_list = glob.glob('cb_acquisitions/*csv')\n",
    "dfs = []\n",
    "for f in acq_file_list:\n",
    "    dfs.append(pd.read_csv(f))\n",
    "acq_df = pd.concat(dfs, ignore_index=True)\n",
    "#print acq_df['Acquired Company Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3729\n",
      "389\n"
     ]
    }
   ],
   "source": [
    "cb_cname = crunch_data['Company Name']\n",
    "acq_cname = acq_df['Acquired Company Name']\n",
    "print len(set(acq_cname))\n",
    "print len(set(cb_cname.values) & set(acq_cname.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print crunch_data.shape\n",
    "# print crunch_data.columns\n",
    "# print acq_df.columns\n",
    "crunch_data_merged = crunch_data.merge(acq_df, how='left', left_on='Company Name', right_on='Acquired Company Name')\n",
    "# print crunch_data_merged.shape\n",
    "# print crunch_data_merged.columns\n",
    "#crunch_data_merged[crunch_data_merged['Acquired Company Name'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dict = dict()\n",
    "df_dict['cb'] = crunch_data_merged\n",
    "# df_dict['cb_acq'] = acq_df\n",
    "df_dict['ks'] = kickstart_data\n",
    "df_dict['al'] = angellist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ks (3988, 12)\n",
      "Index([u'active', u'byline', u'city', u'currency', u'description', u'end_date',\n",
      "       u'funded_amt', u'funded_pct', u'headline', u'state', u'title', u'url'],\n",
      "      dtype='object')\n",
      "cb (11931, 33)\n",
      "Index([u'Company Name', u'Category Groups', u'Headquarters Location',\n",
      "       u'Description', u'Crunchbase Rank', u'Founded Date',\n",
      "       u'Number of Articles', u'Total Equity Funding Amount',\n",
      "       u'Total Funding Amount', u'Closed Date', u'Categories', u'Status',\n",
      "       u'Number of Founders', u'Number of Employees', u'Last Funding Date',\n",
      "       u'Last Funding Amount', u'Last Equity Funding Amount',\n",
      "       u'Number of Investors', u'IPO Date', u'Money Raised at IPO',\n",
      "       u'Valuation at IPO', u'Stock Symbol', u'Stock Exchange',\n",
      "       u'Trend Score (7 Days)', u'Trend Score (30 Days)',\n",
      "       u'Trend Score (90 Days)', u'Website', u'year', u'Transaction Name',\n",
      "       u'Acquired Company Name', u'Acquiring Company Name',\n",
      "       u'Announced On Date', u'Price'],\n",
      "      dtype='object')\n",
      "al (6194, 13)\n",
      "Index([u'al_link', u'featured', u'joined_date', u'location', u'market',\n",
      "       u'product_desc', u'raised', u'signal', u'size', u'stage', u'title',\n",
      "       u'website', u'score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for key, item in df_dict.iteritems():\n",
    "    print key, item.shape\n",
    "    print item.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2016\n",
       "1         2016\n",
       "2         2016\n",
       "3         2012\n",
       "4         2012\n",
       "5         2013\n",
       "6         2012\n",
       "7         2012\n",
       "8         2012\n",
       "9         2012\n",
       "10        2015\n",
       "11        2013\n",
       "12        2012\n",
       "13        2012\n",
       "14        2012\n",
       "15        2012\n",
       "16        2013\n",
       "17        2012\n",
       "18        2014\n",
       "19        2014\n",
       "60        2012\n",
       "61        2013\n",
       "62        2015\n",
       "63        2012\n",
       "64        2012\n",
       "65        2013\n",
       "66        2012\n",
       "67        2012\n",
       "68        2011\n",
       "69        2011\n",
       "          ... \n",
       "309807    2014\n",
       "309808    2015\n",
       "309851    2014\n",
       "309854    2013\n",
       "309855    2015\n",
       "309862    2015\n",
       "309864    2012\n",
       "309935    2015\n",
       "309942    2013\n",
       "309943    2014\n",
       "309947    2015\n",
       "310031    2015\n",
       "310032    2015\n",
       "310037    2015\n",
       "310038    2015\n",
       "310039    2014\n",
       "310040    2016\n",
       "328806    2013\n",
       "328889    2014\n",
       "328941    2013\n",
       "329013    2014\n",
       "329117    2015\n",
       "329616    2012\n",
       "334073    2012\n",
       "334085    2014\n",
       "352912    2013\n",
       "357763    2010\n",
       "357773    2011\n",
       "358013    2012\n",
       "363785    2014\n",
       "Name: joined_date, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd=pd.to_datetime(df_dict['al']['joined_date'])\n",
    "dd.map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dict['ks']['data_source'] = 'kickstarter'\n",
    "df_dict['ks'].rename(columns={'description':'description', 'title': 'company_name'},inplace=True)\n",
    "\n",
    "df_dict['cb']['data_source'] = 'crunchbase'\n",
    "df_dict['cb'].rename(columns={'Description':'description', 'Company Name': 'company_name'},inplace=True)\n",
    "df_dict['cb']['acquired'] = df_dict['cb']['Acquired Company Name'].notnull().astype(int)\n",
    "\n",
    "df_dict['al']['data_source'] = 'angellist'\n",
    "df_dict['al'].rename(columns={'product_desc':'description', 'title': 'company_name'},inplace=True)\n",
    "df_dict['al']['acquired'] = (df_dict['al']['stage']=='Acquired').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, item in df_dict.iteritems():\n",
    "    print key, item.shape\n",
    "    print item.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge_data=pd.concat([x for _, x in df_dict.iteritems()], ignore_index=True)\n",
    "print merge_data.shape\n",
    "merge_data.drop_duplicates(inplace=True)\n",
    "print merge_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge_data.data_source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print merge_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge_data_select = merge_data[['company_name', 'description', 'data_source', 'acquired']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge_data_select[merge_data_select['company_name']=='The']\n",
    "merge_data_select = merge_data_select[(merge_data_select.data_source=='crunchbase') | (merge_data_select.data_source=='angellist')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge_data_select.company_name.value_counts()\n",
    "merge_data_select = merge_data_select.drop_duplicates()\n",
    "merge_data_select['description']=merge_data_select['description'].str.replace('\\n',' ')\n",
    "# merge_data['Description']=merge_data['Description'].str.replace('\\\\',' ')\n",
    "\n",
    "merge_data_select=merge_data_select.dropna(subset=['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "stop_list=stopwords.words('english')+['help','new', 'way', 'world', 'make', 'people']\n",
    "\n",
    "import itertools\n",
    "palette = itertools.cycle(sns.hls_palette(15))\n",
    "markers= itertools.cycle(['x','o','v','^','<','s'])\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words=20):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in token_pattern.findall(doc)]\n",
    "    \n",
    "def plot_top_words(model, feature_names, topic_idx, n_top_words=10):\n",
    "#     for topic_idx, topic in enumerate(model.components_):\n",
    "    topic=normalize(model.components_[topic_idx].reshape(1,-1))[0]\n",
    "    sorted_idx=topic.argsort()[:-n_top_words - 1:-1]\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    features=[feature_names[i] for i in sorted_idx]\n",
    "    print(\" \".join(features))\n",
    "    ax = sns.barplot(x=range(10),y=topic[sorted_idx])\n",
    "    feature_id=0\n",
    "    height= max(topic[sorted_idx])*0.6\n",
    "\n",
    "    for p in ax.patches:\n",
    "        ax.text(p.get_x()+0.15, height-0.1, '{}'.format(features[feature_id]))\n",
    "        feature_id+=1\n",
    "\n",
    "def plot_cluster(model, features, tsne_features, feature_names, nmf_model):\n",
    "    \"\"\"\n",
    "    model: the clustering model. model.labels_ should contain the clusting label\n",
    "    features: feature vectors used to compute the clustering centroid\n",
    "    tsne_features: feature vectors for plot with tsne\n",
    "    \"\"\"\n",
    "    cluster_labels=model.labels_\n",
    "    n_top_words=10\n",
    "    num_clusters=max(cluster_labels)+1\n",
    "    cluster_name=[]\n",
    "    # plotting\n",
    "    # sample 2000 data points for scattering plot\n",
    "    sample_for_plt, label_for_plt=resample(tsne_features,cluster_labels,n_samples=2000,random_state=0)\n",
    "    mapping = TSNE(n_components=2,init='random', random_state=0, n_iter=2000, verbose=0, learning_rate=100, perplexity=20)\n",
    "    embed=mapping.fit_transform(sample_for_plt)\n",
    "    fig=figure(figsize=(15,7))\n",
    "    # plot the clusters\n",
    "    subplot(1,2,1)\n",
    "    for i in range(num_clusters):\n",
    "        subgroup=embed[label_for_plt==i,:]\n",
    "        scatter(subgroup[:,0],subgroup[:,1],s=15, color=next(palette), marker=next(markers), label='{}'.format(i))\n",
    "    legend()\n",
    "    # plot the histgram of the clusters \n",
    "    subplot(1,2,2)\n",
    "    hist(cluster_labels)\n",
    "    # print out the cluster centers\n",
    "    fig=figure(figsize=(15,10))\n",
    "    for i in range(max(cluster_labels)+1):\n",
    "        cluster=features[cluster_labels==i,:]\n",
    "        centroid=np.mean(cluster,axis=0)\n",
    "        centroid_feature=centroid.dot(nmf_model.components_)\n",
    "        sorted_idx=centroid_feature.argsort()[:-n_top_words - 1:-1]\n",
    "        c_name=' '.join([feature_names[j] for j in sorted_idx[:3]])\n",
    "        cluster_name.append(c_name)\n",
    "        subplot(num_clusters,1,i+1)\n",
    "        ax = sns.barplot(x=range(10),y=centroid_feature[sorted_idx])\n",
    "        top_words=[feature_names[j] for j in sorted_idx]\n",
    "        word_id=0\n",
    "        height= max(centroid_feature[sorted_idx])*0.6\n",
    "        for p in ax.patches:\n",
    "            ax.text(p.get_x()+0.15, height, '{}'.format(top_words[word_id]))\n",
    "            word_id+=1\n",
    "    return cluster_name\n",
    "\n",
    "def print_cluster_member(model, cluster_id, data, num_samples=10):\n",
    "    cluster_member=data.iloc[model.labels_==cluster_id]\n",
    "    print 'number of companies in this class:', len(cluster_member)\n",
    "    for m in cluster_member.head(num_samples).iterrows():\n",
    "        print 'company ID:', m[0]\n",
    "        print m[1]['Description']\n",
    "\n",
    "def cluster_companies(df_data, description_column_name, num_clusters):\n",
    "    \"\"\"\n",
    "    df_data: company dataframe\n",
    "    description_column_name: column name of the dataframe corresponding to company description\n",
    "    num_clusters: number of clusters \n",
    "    \"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=2,\n",
    "                                       max_features=3000,\n",
    "                                       tokenizer=LemmaTokenizer(),\n",
    "                                       stop_words=stop_list)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(df_data[description_column_name])\n",
    "    nmf = NMF(n_components=15, random_state=1, \n",
    "              alpha=.1, l1_ratio=.2, max_iter=5000).fit(tfidf)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    topic_feature=nmf.transform(tfidf)\n",
    "    topic_feature_norm=normalize(topic_feature)\n",
    "    agg= AgglomerativeClustering(n_clusters=num_clusters, linkage='ward')\n",
    "    agg.fit(topic_feature_norm)\n",
    "    df_data['cluster_id']=agg.labels_\n",
    "    cluster_name= plot_cluster(agg, topic_feature, topic_feature_norm, tfidf_feature_names, nmf)\n",
    "    df_data['cluster_name']= df_data['cluster_id'].apply(lambda x: cluster_name[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_companies(df_data=merge_data_select, description_column_name='description', num_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge_data_select.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge_data_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = merge_data_select.groupby('cluster_name')['acquired'].sum()\n",
    "#df1['count'] = merge_data_select.groupby('cluster_name').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = merge_data_select.groupby('cluster_name')['acquired'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acq_ration = (df1/df2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print acq_ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
